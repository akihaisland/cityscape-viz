{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25081e0f-16d8-4579-b882-35fc92cd5dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from constants import complete_city_list\n",
    "\n",
    "# following the expert knowledge from xinghan, we divided all features into 5 groups, see the following:\n",
    "feature_list_0 = ['vegetation', 'Street_Scale']\n",
    "feature_list_1 = ['B', 'C', 'G', 'M', 'O', 'P', 'S', 'W']\n",
    "feature_list_2 = ['Art_Deco', 'Brutalism', 'Eastern_Asian_Regional', 'Eastern_European_Regional', 'Georgian', 'Greystone', 'High-tech', \\\n",
    "                'International', 'Middle_Eastern_Regional',\n",
    "                'Modern_high-rise_Apartment', 'Neoclassical', 'Nordic_Regional',\n",
    "                'Postmodern', 'Ranch-style', 'Scandinavian_Vernacular',\n",
    "                'Southeast_Asian_Regional', 'Southern_Asian_Regional',\n",
    "                'Southern_European_Regional', 'Tube-shaped_Apartment', 'Victorian',\n",
    "                'Western_European_Vernacular', 'Worker_Cottage']\n",
    "general_feature_list = feature_list_0 + feature_list_1 + feature_list_2\n",
    "\n",
    "# color feature\n",
    "# process the origin feature file, deleting unnecessary columns and preserving the feature columns \n",
    "feature_list_3 = pd.read_csv('/hpc2ssd/JH_DATA/spooler/xzeng159/data/banner/Amsterdam.csv').columns.drop(['ID', 'lat', 'lon', 'area']).tolist()\n",
    "feature_list_4 = pd.read_csv('/hpc2ssd/JH_DATA/spooler/xzeng159/data/color/Amsterdam.csv').columns.drop(['ID', 'Lat', 'Lon']).tolist()\n",
    "\n",
    "all_feature_list = general_feature_list + feature_list_3 + feature_list_4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2e70ab-d738-44a2-924c-b1d0ea718ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义append_prob_matrix函数\n",
    "def append_prob_matrix(df, super_set):\n",
    "    df = df.reset_index(drop=True)  # 重置索引，确保索引是唯一的\n",
    "    prob_matrix_df = pd.DataFrame(\n",
    "        0, index=np.arange(len(df)), columns=super_set)\n",
    "    for i, row in df.iterrows():\n",
    "        col_name_0 = f'{row[\"cls1\"]}'\n",
    "        col_name_1 = f'{row[\"cls2\"]}'\n",
    "        prob_matrix_df.at[i, col_name_0] = row['prob1']\n",
    "        prob_matrix_df.at[i, col_name_1] = row['prob2']\n",
    "    result_df = pd.concat([df, prob_matrix_df], axis=1)\n",
    "    return result_df  \n",
    "\n",
    "\n",
    "def load_and_merge_city_data(city_name, base_folder):\n",
    "    \"\"\"\n",
    "    对于给定的城市名称，加载并合并该城市的general, color, banner数据。\n",
    "    \n",
    "    Parameters:\n",
    "    - city_name: str, 城市名称。\n",
    "    - base_folder: str, 包含general, color, banner文件夹的根目录路径。\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame, 合并后的城市数据。\n",
    "    \"\"\"\n",
    "    # 定义一个空DataFrame作为合并后的城市数据\n",
    "    merged_city_data = pd.DataFrame()\n",
    "    \n",
    "    # 遍历每个特征文件夹\n",
    "    for feature in ['general', 'color', 'banner']:\n",
    "        folder_path = os.path.join(base_folder, feature)\n",
    "        file_path = os.path.join(folder_path, f'{city_name}.csv')\n",
    "        \n",
    "        # 检查文件是否存在\n",
    "        if os.path.exists(file_path):\n",
    "            temp_data = pd.read_csv(file_path)\n",
    "            if feature == 'general':\n",
    "                temp_data.drop(['lat', 'lon'], axis=1, inplace=True)\n",
    "                temp_data = append_prob_matrix(temp_data, feature_list_2)\n",
    "                temp_data = temp_data[['ID'] + general_feature_list]\n",
    "                temp_data.dropna(inplace=True)\n",
    "            elif feature == 'color':\n",
    "               temp_data.drop(['Lat', 'Lon'], axis=1, inplace=True) # for color\n",
    "               temp_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "               temp_data.dropna(inplace=True)\n",
    "            elif feature == 'banner':\n",
    "                temp_data.drop(['lat', 'lon', 'area'], axis=1, inplace=True) \n",
    "                temp_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "                temp_data.dropna(inplace=True)\n",
    "            # 如果是第一个特征，直接赋值给merged_city_data\n",
    "            if merged_city_data.empty:\n",
    "                merged_city_data = temp_data\n",
    "            else:\n",
    "                # 使用ID列进行合并，为重复列添加适当后缀\n",
    "                merged_city_data = pd.merge(merged_city_data, temp_data, on='ID', how='left')\n",
    "        else:\n",
    "            print(f\"未找到文件: {file_path}\")\n",
    "        \n",
    "        if feature == 'banner':\n",
    "            merged_city_data['city'] = city_name\n",
    "    merged_city_data.dropna(inplace=True)\n",
    "    print(f\"{city_name} finished\")\n",
    "    return merged_city_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7687bf-f8bb-4966-b450-3c8c4451d65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "base_folder = '/hpc2ssd/JH_DATA/spooler/xzeng159/data'\n",
    "\n",
    "city_list = complete_city_list\n",
    "\n",
    "def process_city(city_name):\n",
    "    print(f\"Processing {city_name}\")\n",
    "    return load_and_merge_city_data(city_name, base_folder)\n",
    "\n",
    "# 使用多进程加速数据加载和合并\n",
    "def load_data_concurrently(city_list, base_folder):\n",
    "    all_data = pd.DataFrame()\n",
    "    with ProcessPoolExecutor(max_workers=16) as executor:  # 可根据核心数调整\n",
    "        for city_data in executor.map(process_city, city_list):\n",
    "            all_data = pd.concat([all_data, city_data], ignore_index=True)\n",
    "    return all_data\n",
    "\n",
    "# 现在，我们使用并行处理版本的函数来加载数据\n",
    "all_data = load_data_concurrently(city_list, base_folder)\n",
    "print(all_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb275d4-3c04-40eb-9347-f113c23a2733",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8739516f-d102-4bf5-9209-dd74969519fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00743f13-64b0-4572-9cef-7748ab737bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_category_mapping(series):\n",
    "    return {category: idx for idx, category in enumerate(series.unique())}\n",
    "\n",
    "# Function to convert dataframe column to tensor of indices\n",
    "\n",
    "def column_to_tensor(df, mapping):\n",
    "    return torch.tensor([mapping[category] for category in df])\n",
    "\n",
    "city_mapping = create_category_mapping(all_data['city'])\n",
    "city_tensor = column_to_tensor(all_data['city'], city_mapping)\n",
    "all_data['city'] = city_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dc97c4-6ad4-4751-b9ed-3c0d98505dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分割特征和标签（假设标签列名为 'label'）\n",
    "X = all_data[all_feature_list].values\n",
    "y = all_data['city'].values\n",
    "\n",
    "# 分割数据集为训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# 获取特征索引\n",
    "group0_indices = [all_feature_list.index(f) for f in feature_list_0]\n",
    "group1_indices = [all_feature_list.index(f) for f in feature_list_1]\n",
    "group2_indices = [all_feature_list.index(f) for f in feature_list_2]\n",
    "group3_indices = [all_feature_list.index(f) for f in feature_list_3]\n",
    "group4_indices = [all_feature_list.index(f) for f in feature_list_4]\n",
    "\n",
    "# 应用索引分割特征\n",
    "X_train_group0 = X_train[:, group0_indices]\n",
    "X_train_group1 = X_train[:, group1_indices]\n",
    "X_train_group2 = X_train[:, group2_indices]\n",
    "X_train_group3 = X_train[:, group3_indices]\n",
    "X_train_group4 = X_train[:, group4_indices]\n",
    "\n",
    "X_test_group0 = X_test[:, group0_indices]\n",
    "X_test_group1 = X_test[:, group1_indices]\n",
    "X_test_group2 = X_test[:, group2_indices]\n",
    "X_test_group3 = X_test[:, group3_indices]\n",
    "X_test_group4 = X_test[:, group4_indices]\n",
    "\n",
    "# 数据预处理\n",
    "scaler0 = StandardScaler()\n",
    "scaler1 = StandardScaler()\n",
    "scaler2 = StandardScaler()\n",
    "scaler3 = StandardScaler()\n",
    "scaler4 = StandardScaler()\n",
    "\n",
    "\n",
    "X_train_group0 = scaler0.fit_transform(X_train_group0)\n",
    "X_train_group1 = scaler1.fit_transform(X_train_group1)\n",
    "X_train_group2 = scaler2.fit_transform(X_train_group2)\n",
    "X_train_group3 = scaler3.fit_transform(X_train_group3)\n",
    "X_train_group4 = scaler4.fit_transform(X_train_group4)\n",
    "\n",
    "\n",
    "X_test_group0 = scaler0.transform(X_test_group0)\n",
    "X_test_group1 = scaler1.transform(X_test_group1)\n",
    "X_test_group2 = scaler2.transform(X_test_group2)\n",
    "X_test_group3 = scaler3.transform(X_test_group3)\n",
    "X_test_group4 = scaler4.transform(X_test_group4)\n",
    "\n",
    "\n",
    "# 转换为Torch tensors\n",
    "train_data = TensorDataset(torch.tensor(X_train_group0, dtype=torch.float), \n",
    "                           torch.tensor(X_train_group1, dtype=torch.float), \n",
    "                           torch.tensor(X_train_group2, dtype=torch.float), \n",
    "                           torch.tensor(X_train_group3, dtype=torch.float), \n",
    "                           torch.tensor(X_train_group4, dtype=torch.float), \n",
    "                           torch.tensor(y_train, dtype=torch.long))\n",
    "test_data = TensorDataset(torch.tensor(X_test_group0, dtype=torch.float), \n",
    "                          torch.tensor(X_test_group1, dtype=torch.float), \n",
    "                          torch.tensor(X_test_group2, dtype=torch.float), \n",
    "                          torch.tensor(X_test_group3, dtype=torch.float), \n",
    "                          torch.tensor(X_test_group4, dtype=torch.float),\n",
    "                          torch.tensor(y_test, dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b5fb0d-f750-4490-bcae-2cd3c1bab7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import MLPEmbedding\n",
    "\n",
    "batch_size = 32768\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "input_sizes = [len(group0_indices), len(group1_indices), len(group2_indices), len(group3_indices), len(group4_indices)]\n",
    "embedding_sizes = [32, 32, 32, 32, 32]  # 假设所有嵌入层的输出尺寸为32\n",
    "hidden_size = 128\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "model = MLPEmbedding(input_sizes, embedding_sizes, hidden_size, num_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c4d6f2-f425-4178-b3e6-9fdf6dd50c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from utils import calculate_accuracy, calculate_metrics\n",
    "\n",
    "# 训练循环\n",
    "epochs = 10\n",
    "num_classes = len(np.unique(y))\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for *groups, labels in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(*groups)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    if (epoch+1) % 5 == 0 and epoch != 0:\n",
    "        train_accuracy = calculate_accuracy(model, train_loader)  # 计算训练集准确率\n",
    "        test_accuracy = calculate_accuracy(model, test_loader)  # 计算测试集准确率\n",
    "        print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}, Train Acc: {train_accuracy*100:.2f}%, Test Acc: {test_accuracy*100:.2f}%')\n",
    "        \n",
    "train_precision, train_recall = calculate_metrics(model, train_loader, num_classes)\n",
    "test_precision, test_recall = calculate_metrics(model, test_loader, num_classes)\n",
    "for i in range(num_classes):\n",
    "    print(f'Class {i} - Train Precision: {train_precision[i]*100:.2f}%, Train Recall: {train_recall[i]*100:.2f}%')\n",
    "    print(f'Class {i} - Test Precision: {test_precision[i]*100:.2f}%, Test Recall: {test_recall[i]*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a0ad9b-afa9-49ab-9584-d31329c73086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory may overflow\n",
    "\n",
    "import gc\n",
    "# del scaler0, scaler1, scaler2, scaler3, scaler4\n",
    "del X_train_group0, X_train_group1, X_train_group2, X_train_group3, X_train_group4\n",
    "del X_test_group0, X_test_group1, X_test_group2, X_test_group3, X_test_group4\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a5694f-10a8-47c1-94da-e4bca45b983d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Confusion Matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def get_predictions_and_labels(model, data_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    labels_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for *groups, labels in data_loader:\n",
    "            outputs = model(*groups)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "            labels_list.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return predictions, labels_list\n",
    "\n",
    "test_predictions, test_labels = get_predictions_and_labels(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56a1c92-fb68-49de-98f8-3b08c2f29b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化混淆矩阵\n",
    "def plot_confusion_matrix(conf_matrix, classes, normalize=False, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        conf_matrix = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "    plt.imshow(conf_matrix, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = conf_matrix.max() / 2.\n",
    "    for i, j in itertools.product(range(conf_matrix.shape[0]), range(conf_matrix.shape[1])):\n",
    "        plt.text(j, i, format(conf_matrix[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if conf_matrix[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c00e1e3-3723-42e2-af1b-ae87eb3b3ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# 生成混淆矩阵\n",
    "conf_matrix = confusion_matrix(test_labels, test_predictions)\n",
    "\n",
    "# 可视化混淆矩阵\n",
    "def plot_confusion_matrix(conf_matrix, classes, normalize=False, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        conf_matrix = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "    plt.imshow(conf_matrix, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = conf_matrix.max() / 2.\n",
    "    for i, j in itertools.product(range(conf_matrix.shape[0]), range(conf_matrix.shape[1])):\n",
    "        plt.text(j, i, format(conf_matrix[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if conf_matrix[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# 将混淆矩阵保存为CSV文件\n",
    "def save_confusion_matrix_to_csv(conf_matrix, class_names, filename):\n",
    "    df_cm = pd.DataFrame(conf_matrix, index=class_names, columns=class_names)\n",
    "    df_cm.to_csv(filename)\n",
    "\n",
    "# 类别名称\n",
    "class_names = [city_list[i] for i in range(num_classes)]\n",
    "\n",
    "# 保存非标准化的混淆矩阵为CSV文件\n",
    "save_confusion_matrix_to_csv(conf_matrix, class_names, './707/confusion_matrix.csv')\n",
    "conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "conf_matrix_normalized = np.round(conf_matrix_normalized, 2)  # 保留两位小数\n",
    "save_confusion_matrix_to_csv(conf_matrix_normalized, class_names, './707/normalize_confusion_matrix.csv')\n",
    "\n",
    "# 画出非标准化的混淆矩阵\n",
    "plt.figure(figsize=(50, 50))\n",
    "plot_confusion_matrix(conf_matrix, classes=class_names, title='Confusion matrix, without normalization')\n",
    "plt.savefig('./707/1.png')\n",
    "\n",
    "# 画出标准化的混淆矩阵\n",
    "plt.figure(figsize=(50, 50))\n",
    "plot_confusion_matrix(conf_matrix, classes=class_names, normalize=True, title='Normalized confusion matrix')\n",
    "plt.savefig('./707/2.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a24af76-d271-4d43-b9a9-ed7463bad3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(data_loader):\n",
    "    all_groups, all_labels = [], []\n",
    "    for *groups, labels in data_loader:\n",
    "        all_groups.append(groups)\n",
    "        all_labels.append(labels)\n",
    "    all_groups = [np.concatenate([group[i].numpy() for group in all_groups], axis=0) for i in range(len(all_groups[0]))]\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    return all_groups, all_labels\n",
    "\n",
    "# 收集所有数据\n",
    "groups, labels = collect_data(train_loader)\n",
    "\n",
    "# 收集中间层输出\n",
    "model.eval()  # 确保模型处于评估模式\n",
    "embeddings = []\n",
    "with torch.no_grad():\n",
    "    for *groups, _ in tqdm(train_loader):\n",
    "        group = [torch.tensor(g, dtype=torch.float32) for g in groups]\n",
    "        embedding = model(*group, return_embedding=True).cpu().numpy()\n",
    "        embeddings.append(embedding)\n",
    "embeddings = np.concatenate(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08a6262-0962-4e40-95da-89980229856f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确保此时 embeddings 和 labels 长度相同\n",
    "assert len(embeddings) == len(labels), \"The embeddings and labels should have the same length.\"\n",
    "\n",
    "# 使用t-SNE进行降维\n",
    "perplexity = 30\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=perplexity)\n",
    "tsne_results = tsne.fit_transform(embeddings)\n",
    "\n",
    "# 可视化\n",
    "class_names = [city_list[i] for i in range(len(np.unique(labels)))]\n",
    "num_classes = len(class_names)\n",
    "plt.figure(figsize=(32,20))\n",
    "for class_idx in range(num_classes):\n",
    "    indices = labels == class_idx\n",
    "    plt.scatter(tsne_results[indices, 0], tsne_results[indices, 1], label=f'{class_names[class_idx]}', alpha=0.1)\n",
    "plt.title(f'All data points -- perplexity: {perplexity}')\n",
    "plt.legend()\n",
    "plt.savefig(f'./520/all_points_perplexity-{perplexity}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e583de1-12fc-4f7a-abba-9e434bf8176a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def sample_data(data_loader, sample_size):\n",
    "    for *groups, labels in data_loader:\n",
    "        sampled_indices = np.random.choice(len(labels), size=sample_size, replace=False)\n",
    "        groups_sampled = [group[sampled_indices] for group in groups]\n",
    "        labels_sampled = labels[sampled_indices]\n",
    "        break  # 只取一个批次进行采样\n",
    "    return groups_sampled, labels_sampled\n",
    "\n",
    "# 采样数据\n",
    "sample_size = 30000  # 设置你希望采样的大小\n",
    "groups_sampled, labels_sampled = sample_data(test_loader, sample_size)\n",
    "\n",
    "# 收集中间层输出\n",
    "model.eval()  # 确保模型处于评估模式\n",
    "with torch.no_grad():\n",
    "    embeddings = model(*groups_sampled, return_embedding=True).cpu().numpy()\n",
    "\n",
    "# 使用t-SNE进行降维\n",
    "perplexity = 20\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=perplexity, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7bae30-e3c5-4adf-9bb4-ed67699567fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 假设以下变量已正确定义和初始化\n",
    "# tsne_results, labels_sampled, sample_size, perplexity\n",
    "class_names = [city_list[i] for i in range(num_classes)]\n",
    "# 独特标签的颜色映射\n",
    "unique_labels = set(culture_group_labels)  # 根据文化组标签创建一个独特标签集\n",
    "colors = plt.cm.rainbow(np.linspace(0, 1, len(unique_labels)))  # 为每个独特标签生成颜色\n",
    "color_map = dict(zip(unique_labels, colors))  # 创建标签到颜色的映射\n",
    "\n",
    "plt.figure(figsize=(32, 20))\n",
    "num_classes = len(np.unique(labels_sampled.numpy()))  # 更新num_classes基于采样数据\n",
    "\n",
    "for class_idx in range(num_classes):\n",
    "    indices = labels_sampled.numpy() == class_idx\n",
    "    label = None\n",
    "    for i in range(len(culture_group)):\n",
    "        if class_names[class_idx] in culture_group[i]:\n",
    "            label = culture_group_labels[i]\n",
    "            break\n",
    "    if label is None:\n",
    "        exit()\n",
    "\n",
    "    # 使用之前创建的颜色映射来确定点的颜色\n",
    "    color = color_map.get(label, 'k')  # 如果label不在color_map中，默认使用黑色\n",
    "    plt.scatter(tsne_results[indices, 0], tsne_results[indices, 1], label=label, color=color, alpha=0.3)\n",
    "\n",
    "# 优化图例，以仅显示一次每个标签\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))  # 移除重复标签\n",
    "plt.legend(by_label.values(), by_label.keys())\n",
    "\n",
    "plt.title(f'Randomly sampled {sample_size} points -- perplexity: {perplexity}')\n",
    "plt.savefig(f'./520/{sample_size}points_perplexity-{perplexity}.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c1823b-8ddf-416b-8ff7-950401101c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "class_names = [city_list[i] for i in range(num_classes)]\n",
    "# 假设以下变量已正确定义和初始化\n",
    "# tsne_results, labels_sampled, sample_size, perplexity, class_names, culture_group_labels\n",
    "\n",
    "df_tsne = pd.DataFrame(tsne_results, columns=['tsne-2d-one', 'tsne-2d-two'])\n",
    "df_tsne['city_label'] = [class_names[i] for i in labels_sampled.numpy()]\n",
    "\n",
    "# 增加一个新的列来存储文化组的标签\n",
    "df_tsne['culture_label'] = None  # 初始化文化组标签列 \n",
    "for i in range(len(culture_group)):\n",
    "    df_tsne.loc[df_tsne['city_label'].isin(culture_group[i]), 'culture_label'] = culture_group_labels[i]\n",
    "\n",
    "# 确保每个城市类别都有一个对应的文化组标签\n",
    "# assert not df_tsne['culture_label'].isnull().any(), \"Some city labels don't have a corresponding culture group label.\"\n",
    "\n",
    "# 使用plotly.express绘制交互式图表，并将颜色设置为文化组标签\n",
    "fig = px.scatter(df_tsne, x='tsne-2d-one', y='tsne-2d-two', color='culture_label',\n",
    "                 title=f'Randomly Sampled {sample_size} Points -- Perplexity: {perplexity}',\n",
    "                 labels={\"culture_label\": \"Culture Group Label\"})\n",
    "\n",
    "fig.update_layout(width=1800, height=1200)\n",
    "# 如果需要保存图表，可以使用write_image方法，但需要额外安装kaleido\n",
    "# fig.write_image(f'./506/{sample_size}points_perplexity-{perplexity}.png')\n",
    "HTML(fig.to_html())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffe3fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.write_html('./test.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e453e1db-e0a4-44ee-8f4e-d38a8f9102d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tsne['label'] = [class_names[i] for i in labels_sampled.numpy()]\n",
    "df_tsne['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85db377d-1a0f-4883-84d4-327b0591eb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_tsne = pd.DataFrame(tsne_results, columns=['tsne-2d-one', 'tsne-2d-two'])\n",
    "df_tsne['label'] = [class_names[i] for i in labels_sampled.numpy()]\n",
    "\n",
    "# 使用plotly.express绘制交互式图表\n",
    "fig = px.scatter(df_tsne, x='tsne-2d-one', y='tsne-2d-two', color='label',\n",
    "                 title=f'Randomly Sampled {sample_size} Points -- Perplexity: {perplexity}',\n",
    "                 labels={\"label\": \"City Label\"})\n",
    "\n",
    "fig.update_layout(width=1800, height=1200)\n",
    "# 如果需要保存图表，可以使用write_image方法，但需要额外安装kaleido\n",
    "# fig.write_image(f'./506/{sample_size}points_perplexity-{perplexity}.png')\n",
    "HTML(fig.to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40656ec2-3891-4e14-b387-4dd490b4faa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.write_html('./test_city.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84116ede",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "a840a2853298d1b874ebe4c4ef9957c573cc928a20642661de194c9d6366f518"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
